"""
Feedback System for Java Peer Review Training System.

This module provides a unified system for displaying feedback on student reviews
and handling the feedback tab functionality with proper workflow state management.
"""

import streamlit as st
import logging
import pandas as pd
import matplotlib.pyplot as plt
import time
import traceback
from typing import List, Dict, Any, Optional, Tuple, Callable
from analytics.badge_manager import BadgeManager
from auth.mysql_auth import MySQLAuthManager
from utils.language_utils import t
from ui.components.comparison_report_renderer import ComparisonReportRenderer
from utils.code_utils import _log_user_interaction_feedback_system

import plotly.express as px
import plotly.graph_objects as go
from analytics.badge_manager import BadgeManager
import datetime
import math

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class FeedbackSystem:
    """
    Unified feedback system for the Java Peer Review Training System.
    
    This class combines the UI rendering and workflow state management
    for the feedback tab. It handles displaying analysis results,
    generating comparison reports, and updating user statistics.
    """
    
    def __init__(self, workflow, auth_ui=None):
        """Initialize the FeedbackSystem with workflow and auth components."""
        self.badge_manager = BadgeManager()
        self.auth_manager = MySQLAuthManager()
        self.workflow = workflow
        self.auth_ui = auth_ui
        self.stats_updates = {}
        self.comparison_renderer = ComparisonReportRenderer()

    def render_feedback_tab(self):
        """Render the feedback tab with review analysis and badge awards."""
        state = st.session_state.workflow_state
        
        # Check if review process is completed
        review_completed = self._check_review_completion(state)
        
        # Block access if review not completed
        if not review_completed:
            self._display_completion_required_message(state)
            return
        
        # Get the latest review analysis and history
        latest_review, review_history = self._extract_review_data(state)
        
        # Generate comparison report if needed
        if latest_review and latest_review.analysis and (not hasattr(state, 'comparison_report') or not state.comparison_report):
            self._generate_comparison_report(state, latest_review,review_history)
            
        # Check if comparison report was generated by the workflow
        comparison_report = getattr(state, 'comparison_report', None)
        
      
        latest_analysis = latest_review.analysis if latest_review else None
        
        # Get user ID from session state
        user_id = st.session_state.auth.get("user_id") if "auth" in st.session_state else None
        
        # Add tabs for different sections of feedback
        st.markdown('<div class="feedback-tabs">', unsafe_allow_html=True)
        feedback_tabs = st.tabs([t("review_feedback"), t("badges")])
        st.markdown('</div>', unsafe_allow_html=True)
        
        with feedback_tabs[0]:
            self.render_newly_awarded_badges(state)
            # Display the feedback results from workflow
            self.render_results(
                comparison_report=comparison_report,
                review_analysis=latest_analysis,
                review_history=review_history
            )
        
        with feedback_tabs[1]:
            # Show user badges
            if user_id:
                self.render_badge_showcase(user_id)
            else:
                st.info("Login to track your badges!")
              
        # Add a button to start a new session
        st.markdown("---")
        self._render_new_session_button()

    def render_results(self, 
                      comparison_report: str = None,                     
                      review_analysis: Dict[str, Any] = None,
                      review_history: List[Dict[str, Any]] = None) -> None:
        """
        Render the analysis results and feedback with improved review visibility.
        
        Args:
            comparison_report: Comparison report text           
            review_analysis: Analysis of student review
            review_history: History of review iterations
        """
      
        if not comparison_report and not review_analysis:
            st.info(t("no_analysis_results"))
            return
        
        # First show performance summary metrics at the top
        if review_history and len(review_history) > 0 and review_analysis:
            self._render_performance_summary(review_analysis, review_history)
        
        # Display the comparison report using the new renderer
        if comparison_report:           
            self.comparison_renderer.render_comparison_report(comparison_report)       
        
        st.markdown("---")             
            
    def _render_performance_summary(self, review_analysis: Dict[str, Any], review_history: List[Dict[str, Any]]):
        """Render enhanced performance summary metrics and charts with CJK font support"""
       
        # Get the correct total_problems count from original_error_count if available
        original_error_count = review_analysis[t('total_problems')]
             
        # Create a progress chart if multiple iterations
        if len(review_history) > 1:
            # Extract data for chart
            iterations = []
            identified_counts = []
            accuracy_percentages = []
            
            for review in review_history:
                analysis = review["review_analysis"]
                iterations.append(review["iteration_number"])
                # Use consistent error count for all iterations
                review_identified = analysis[t("identified_count")]
                identified_counts.append(review_identified)
                
                # Calculate accuracy consistently
                review_accuracy = (review_identified / original_error_count * 100) if original_error_count > 0 else 0
                accuracy_percentages.append(review_accuracy)
                    
            # Create a DataFrame for the chart
            chart_data = pd.DataFrame({
                t("iteration"): iterations,
                t("issues_found"): identified_counts,
                f"{t('accuracy')} (%)": accuracy_percentages
            })
            
            # Display the chart with two y-axes
            st.subheader(t("progress_across_iterations"))
            
            # Set font configuration for matplotlib for CJK support
            import matplotlib
            import matplotlib.font_manager as fm
            
            # Configure matplotlib to use a font that supports CJK characters
            # Try to find appropriate fonts based on platform
            font_list = matplotlib.rcParams['font.sans-serif']
            
            # Add CJK compatible fonts to the front of the list
            # Common CJK compatible fonts across different platforms
            cjk_fonts = ['Noto Sans CJK JP', 'Noto Sans CJK TC', 'Noto Sans CJK SC', 
                        'Microsoft YaHei', '微软雅黑', 'Microsoft JhengHei', '微軟正黑體',
                        'SimHei', '黑体', 'WenQuanYi Zen Hei', 'WenQuanYi Micro Hei',
                        'Hiragino Sans GB', 'STHeiti', 'Source Han Sans CN', 
                        'Source Han Sans TW', 'Source Han Sans JP',
                        'DroidSansFallback', 'Droid Sans Fallback']
            
            # Set the font family to ensure CJK support
            matplotlib.rcParams['font.family'] = 'sans-serif'
            matplotlib.rcParams['font.sans-serif'] = cjk_fonts + font_list
            
            # Use a non-DejaVu font explicitly for this plot
            try:
                # Trying to use a specific CJK-compatible font if available
                plt.rcParams['font.sans-serif'] = cjk_fonts
                # Set Unicode fallback font
                plt.rcParams['axes.unicode_minus'] = False
            except Exception as e:
                # Log the error but continue
                logger.warning(f"Could not set matplotlib font: {str(e)}")
            
            # Using matplotlib for more control - with CJK font configuration
            fig, ax1 = plt.subplots(figsize=(10, 4))
            
            color = 'tab:blue'
            ax1.set_xlabel(t('iteration'))
            ax1.set_ylabel(t('issues_found'), color=color)
            ax1.plot(chart_data[t("iteration")], chart_data[t("issues_found")], marker='o', color=color)
            ax1.tick_params(axis='y', labelcolor=color)
            ax1.grid(True, linestyle='--', alpha=0.7)
            
            ax2 = ax1.twinx()  # Create a second y-axis
            color = 'tab:red'
            ax2.set_ylabel(f"{t('accuracy')} (%)", color=color)
            ax2.plot(chart_data[t("iteration")], chart_data[f"{t('accuracy')} (%)"], marker='s', color=color)
            ax2.tick_params(axis='y', labelcolor=color)
            
            fig.tight_layout()
            st.pyplot(fig)
    
    def _check_review_completion(self, state) -> bool:
        """
        Determine if the review process is completed.
        
        Args:
            state: The workflow state
            
        Returns:
            bool: True if review is completed, False otherwise
        """
        review_completed = False
        
        # Check if max iterations reached or review is sufficient
        if hasattr(state, 'current_iteration') and hasattr(state, 'max_iterations'):
            if getattr(state, 'current_iteration', 0) > getattr(state, 'max_iterations', 3):
                review_completed = True
                logger.debug(t("review_completed_max_iterations"))
            elif getattr(state, 'review_sufficient', False):
                review_completed = True
                logger.debug(t("review_completed_sufficient"))
        
        # Check for all errors identified - HIGHEST PRIORITY CHECK
        if hasattr(state, 'review_history') and state.review_history and len(state.review_history) > 0:
            latest_review = state.review_history[-1]
            analysis = latest_review.analysis if hasattr(latest_review, 'analysis') else {}
            identified_count = analysis[t('identified_count')]
            total_problems = analysis[t('total_problems')]
            
            if identified_count == total_problems and total_problems > 0:
                review_completed = True
                logger.debug(f"{t('review_completed_all_identified')} {total_problems} {t('issues')}")
                
        return review_completed
    
    def _display_completion_required_message(self, state):
        """Display message requiring completion of review before viewing feedback."""
        st.warning(f"{t('complete_all_review_attempts_before_feedback')}")
        st.info(f"{t('current_process_review1')} {getattr(state, 'current_iteration', 1)-1}/{getattr(state, 'max_iterations', 3)} {t('current_process_review2')}")
    
    def _extract_review_data(self, state):
        """
        Extract review data from the workflow state.
        
        Args:
            state: The workflow state
            
        Returns:
            Tuple: (latest_review, review_history)
        """
        latest_review = None
        review_history = []
        
        # Make sure we have review history
        if hasattr(state, 'review_history') and state.review_history:
            review_history_list = state.review_history           
            if review_history_list and len(review_history_list) > 0:
                latest_review = review_history_list[-1]
                
                # Convert review history to the format expected by FeedbackDisplayUI
                for review in review_history_list:                   
                    review_history.append({
                        "iteration_number":  getattr(review, "iteration_number", 0),
                        "student_review": getattr(review, "student_review", ""),
                        "review_analysis": getattr(review, "analysis", {})
                    })
                
        return latest_review, review_history
    
    def _generate_comparison_report(self, state, latest_review, review_history):
        """
        Generate a comparison report for the review feedback.
        
        Args:
            state: The workflow state
            latest_review: The latest review attempt
        """
        try:
            # Get the known problems from the evaluation result instead of code_snippet.known_problems
            if hasattr(state, 'evaluation_result') and state.evaluation_result and 'found_errors' in state.evaluation_result:
                stamp = state.evaluation_result
                found_errors = stamp[t("found_errors")]
                
                # Get the evaluator from the workflow
                evaluator = self.workflow.workflow_nodes.evaluator
                
                if evaluator:
                    # Generate a comparison report using the evaluator's method
                    state.comparison_report = evaluator.generate_comparison_report(
                        found_errors,
                        latest_review.analysis,
                        review_history
                    )
                    logger.debug(t("generated_comparison_report"))
                else:
                    logger.error("Evaluator not available for generating comparison report")
                    state.comparison_report = (
                        f"# {t('review_feedback')}\n\n"
                        f"{t('error_generating_report')} "
                        f"{t('check_review_history')}."
                    )
        except Exception as e:
            logger.error(f"{t('error')} {t('generating_comparison_report')}: {str(e)}")
            logger.error(traceback.format_exc())  # Log full stacktrace
            if not hasattr(state, 'comparison_report') or not state.comparison_report:
                state.comparison_report = (
                    f"# {t('review_feedback')}\n\n"
                    f"{t('error_generating_report')} "
                    f"{t('check_review_history')}."
                )
              
    def _render_new_session_button(self):
        """Render button to start a new session."""
        st.markdown("---")
        new_session_col1, new_session_col2 = st.columns([3, 1])
        with new_session_col1:
            st.markdown(f"### {t('new_session')}")
            st.markdown(t("new_session_desc"))
        with new_session_col2:
            if st.button(t("start_new_session"), use_container_width=True):
                # Clear all update keys in session state
                keys_to_remove = [k for k in st.session_state.keys() if k.startswith("stats_updated_")]
                for key in keys_to_remove:
                    del st.session_state[key]                   
                self.stats_updates = {}                    
                # Set the full reset flag
                st.session_state.full_reset = True
                st.rerun()

    def render_badge_showcase(self, user_id: str) -> None:
        """
        Render the user's earned badges in a gallery-like view organized by category.
        
        Args:
            user_id: The user's ID
        """
        badge_manager = BadgeManager()
        badges = badge_manager.get_user_badges(user_id)
        
        if not badges:
            st.info(f"{t('no_badges_earned')}")
            return
        
        st.subheader(f"🏆 {t('achievement_badges')}")
        
        # Group badges by category and sort
        badge_categories = {}
        for badge in badges:
            category = badge.get("category", "Other")
            if category not in badge_categories:
                badge_categories[category] = []
            badge_categories[category].append(badge)
        
        # Sort categories for consistent display
        sorted_categories = sorted(badge_categories.items())
        
        # Display badges by category in a single gallery view
        if sorted_categories:
            st.markdown('<div class="badge-gallery-container">', unsafe_allow_html=True)
            
            for category, category_badges in sorted_categories:
                badge_count = len(category_badges)
                if badge_count == 1:
                    grid_class = "badge-grid-1"
                elif badge_count == 2:
                    grid_class = "badge-grid-2"
                elif badge_count <= 4:
                    grid_class = "badge-grid-3"
                else:
                    grid_class = "badge-grid-4"
                
                # Create badge grid
                st.markdown(f'<div class="badge-grid {grid_class}">', unsafe_allow_html=True)
                
                # Display badges
                for badge in category_badges:
                    # Format date
                    awarded_at_value = badge.get("awarded_at", "")
                    if isinstance(awarded_at_value, datetime.datetime):
                        awarded_at = awarded_at_value.strftime("%Y-%m-%d")
                    else:
                        awarded_at = str(awarded_at_value).split(' ')[0] if awarded_at_value else ""
                    
                    # Badge card
                    st.markdown(f'''
                    <div class="badge-card">
                        <div class="badge-icon">{badge.get("icon", "🏅")}</div>
                        <div class="badge-name">{badge.get("name", "Badge")}</div>
                        <div class="badge-description">{badge.get("description", "")}</div>
                        <div class="badge-date">📅 {awarded_at}</div>
                    </div>
                    ''', unsafe_allow_html=True)
                
                st.markdown('</div>', unsafe_allow_html=True)  # Close badge-grid
                
                # Add spacing between categories (except for the last one)
                if category != sorted_categories[-1][0]:
                    st.markdown('<div class="badge-category-spacer"></div>', unsafe_allow_html=True)
            
            st.markdown('</div>', unsafe_allow_html=True)  # Close badge-gallery-container
        
        # Add summary statistics
        total_badges = len(badges)
        total_categories = len(badge_categories)
        
        if user_id:
            _log_user_interaction_feedback_system(
                user_id=user_id,
                interaction_category="practice",
                interaction_type="view_badge_showcase",
                details={"badges_count": len(badges)}
            )

        st.markdown(f'''
        <div class="badge-summary">
            <div class="badge-summary-title">🎯 Badge Collection Summary:</div>
            <div class="badge-summary-stats">{total_badges} total badges earned across {total_categories} categories</div>
        </div>
        ''', unsafe_allow_html=True)

    def render_newly_awarded_badges(self, state):
        """Render newly awarded badges section."""
        try:
            if not hasattr(state, 'badge_awards') or not state.badge_awards:
                return
            
            badge_awards = state.badge_awards
            awarded_badges = badge_awards.get('awarded_badges', [])
            points_awarded = badge_awards.get('points_awarded', 0)
            
            if not awarded_badges and points_awarded == 0:
                return
            
            # Add CSS for badge celebration
            st.markdown("""
            <style>
            .badge-celebration-container {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 2rem;
                border-radius: 12px;
                margin: 2rem 0;
                text-align: center;
            }
            .new-badge-card {
                background: linear-gradient(145deg, #f0f0f0, #ffffff);
                border-radius: 12px;
                padding: 1.5rem;
                text-align: center;
                box-shadow: 0 4px 8px rgba(0,0,0,0.1);
                margin: 1rem 0;
                border: 2px solid #ffd700;
            }
            .badge-icon-large { font-size: 3rem; margin-bottom: 0.5rem; }
            .badge-name-large { font-weight: bold; font-size: 1.2rem; color: #333; }
            .points-award-summary {
                background: #28a745;
                color: white;
                padding: 1rem;
                border-radius: 8px;
                text-align: center;
                margin: 1rem 0;
            }
            </style>
            """, unsafe_allow_html=True)
            
            if awarded_badges:
                st.markdown(f"""
                <div class="badge-celebration-container">
                    <h3>🎉 {t('new_badges_earned')}!</h3>
                    <p>Congratulations on earning {len(awarded_badges)} new badges!</p>
                </div>
                """, unsafe_allow_html=True)
                
                # Display new badges
                cols = st.columns(min(len(awarded_badges), 3))
                for i, badge in enumerate(awarded_badges):
                    with cols[i % 3]:
                        st.markdown(f"""
                        <div class="new-badge-card">
                            <div class="badge-icon-large">{badge.get('icon', '🏅')}</div>
                            <div class="badge-name-large">{badge.get('name', 'New Badge')}</div>
                            <div class="badge-description-small">{badge.get('description', '')}</div>
                        </div>
                        """, unsafe_allow_html=True)
                
                st.balloons()  # Celebration effect
            
            if points_awarded > 0:
                st.markdown(f"""
                <div class="points-award-summary">
                    <span>⭐ Points Earned: <strong>{points_awarded}</strong></span>
                </div>
                """, unsafe_allow_html=True)
                
        except Exception as e:
            logger.error(f"Error rendering newly awarded badges: {str(e)}")

def render_feedback_tab(workflow, auth_ui=None):
        """
        Enhanced feedback tab that properly handles both regular and practice sessions.
        """
        from ui.components.feedback_system import FeedbackSystem
        
        # Check if this is a practice session
        practice_session = st.session_state.get("practice_session_active", False)
        practice_error_name = st.session_state.get("practice_error_name", "")
        
        if practice_session:
            # Special header for practice sessions
            st.markdown(f"""
            <div style="background: linear-gradient(90deg, #2196F3, #1976D2); color: white; padding: 1.5rem; border-radius: 8px; margin-bottom: 2rem;">
                <h2 style="margin: 0; color: white;">🎯 Practice Session Complete: {practice_error_name}</h2>
                <p style="margin: 0.5rem 0 0 0; color: white;">Here's your detailed feedback on practicing with this specific error type.</p>
            </div>
            """, unsafe_allow_html=True)
            
            # Add practice-specific information
            col1, col2 = st.columns([3, 1])
            
            with col1:
                st.markdown(f"""
                ### 📚 What You Practiced
                - **Error Type**: {practice_error_name}
                - **Session Type**: Targeted Practice (Single Error Focus)
                - **Learning Goal**: Identify and understand this specific error pattern
                """)
            
            with col2:
                # Option to practice again or try a different error
                if st.button("🔄 Practice Another Error", help="Go back to Error Explorer"):
                    st.session_state.active_tab = 5  # Error Explorer tab
                    st.rerun()
                
                if st.button("🏠 Return to Main Workflow", help="End practice and return to regular workflow"):
                    # Clear practice session flags
                    if "practice_session_active" in st.session_state:
                        del st.session_state["practice_session_active"]
                    if "practice_error_name" in st.session_state:
                        del st.session_state["practice_error_name"]
                    
                    st.session_state.active_tab = 0
                    st.rerun()
            
            st.markdown("---")
        
        # Create the feedback system instance and render
        feedback_system = FeedbackSystem(workflow, auth_ui)
        feedback_system.render_feedback_tab()
        
        # Add practice session completion actions
        if practice_session:
            st.markdown("---")
            st.markdown("### 🎯 Practice Session Actions")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("📚 Practice Same Error Again", use_container_width=True):
                    # Reset the workflow state but keep the same error
                    if hasattr(st.session_state, 'workflow_state') and st.session_state.workflow_state:
                        current_state = st.session_state.workflow_state
                        if hasattr(current_state, 'selected_specific_errors') and current_state.selected_specific_errors:
                            # Get the current error for re-practice
                            current_error = current_state.selected_specific_errors[0]
                            
                            # Reset state for new practice
                            from state_schema import WorkflowState
                            new_state = WorkflowState()
                            new_state.selected_specific_errors = [current_error]
                            new_state.difficulty_level = current_error.get('difficulty_level', 'medium')
                            new_state.code_length = "medium"
                            new_state.error_count_start = 1
                            new_state.error_count_end = 1
                            
                            st.session_state.workflow_state = new_state
                            st.session_state.active_tab = 1  # Go to generate tab
                            st.rerun()
            
            with col2:
                if st.button("🔍 Explore Related Errors", use_container_width=True):
                    st.session_state.active_tab = 5  # Go to Error Explorer
                    st.rerun()
            
            with col3:
                if st.button("🏆 View Progress Dashboard", use_container_width=True):
                    st.session_state.active_tab = 4  # Go to Dashboard
                    st.rerun()

